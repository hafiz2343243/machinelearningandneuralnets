# Decision Trees and the Role of Gini vs Entropy

This repository contains the tutorial and implementation for:  
**Decision Trees and the Role of Gini vs Entropy**  
Prepared for the Machine Learning Techniques module led by Peter Scicluna.

---

## Tutorial Overview

This tutorial explores how decision trees function as a supervised learning algorithm and examines the use of two splitting criteria:

- Gini Impurity
- Information Gain (Entropy)

The tutorial provides:

- A clear conceptual explanation of how decision trees split data
- A comparison between Gini and Entropy criteria
- Visual examples of tree structures
- Python implementation using `scikit-learn`
- Analysis of tree performance and interpretability

---

## Repository Contents

| File/Folder         | Description                                                   |
|---------------------|---------------------------------------------------------------|
| machinelearning.ipynb`   | Jupyter notebook with the full tutorial and working example   |
| `README.md`         | This documentation file with setup, explanation, and licensing|
| `LICENSE`           | MIT License for open reuse and modification                   |
| `images/` (optional)| Folder containing visualizations and output figures           |

---

## How to Run the Tutorial

### Requirements

Install the required Python packages:

```bash
pip install numpy pandas matplotlib seaborn scikit-learn jupyter
